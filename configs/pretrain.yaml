# SSL Pretraining Configuration
#
# Unified config for all SSL paradigms. Pick paradigm with ssl= override.
# Paradigm and experiment_name are auto-derived from ssl.name in pretrain.py.
#
# Usage:
#   uv run python scripts/training/pretrain.py dataset=miiv              # MAE (default)
#   uv run python scripts/training/pretrain.py dataset=miiv ssl=jepa
#   uv run python scripts/training/pretrain.py dataset=miiv ssl=contrastive
#   uv run python scripts/training/pretrain.py dataset=eicu ssl=mae
#   uv run python scripts/training/pretrain.py dataset=miiv ssl=smart model=smart

defaults:
  - data: ricu
  - model: observation_transformer
  - ssl: mae
  - _self_

# Project info
project_name: slices

# Dataset identifier â€” drives data paths via ricu.yaml interpolation
dataset: miiv  # miiv | eicu | combined

# Experiment sprint (for W&B filtering, e.g. sprint=1)
sprint: null

# Random seed for reproducibility
seed: 42

# Output directory (Hydra will create timestamped subdirs)
output_dir: ${hydra:runtime.output_dir}
checkpoint_dir: ${output_dir}/checkpoints

# Data configuration overrides
# Base settings inherited from configs/data/ricu.yaml
data:
  # Increase workers for pretraining (larger batches)
  num_workers: 8

  # Sliding window configuration for longer sequences
  # Enable after re-extracting with longer seq_length_hours (e.g., 168h)
  enable_sliding_windows: false  # Set true after re-extraction
  window_size: 48                # Window size in hours (48h = typical ICU prediction horizon)
  window_stride: 24              # Stride for overlapping windows (50% overlap)

# Encoder overrides (base from configs/model/observation_transformer.yaml: d=128, L=4, H=8)
encoder:
  pooling: none  # Required for SSL - need per-token outputs (vs 'mean' for tasks)

# Training configuration (same budget across thesis paradigms for fair comparison)
training:
  max_epochs: 500
  batch_size: 256

  # Compute settings
  accelerator: auto  # auto | cpu | gpu | mps
  devices: auto  # auto | 1 | [0,1] | etc.
  precision: 32  # 32 | 16 | bf16

  # Regularization
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1

  # Early stopping
  early_stopping_patience: 10

  # Debug: overfit on N batches (0 = disabled)
  overfit_batches: 0

# Optimizer configuration
optimizer:
  name: adamw
  lr: 1.0e-3
  weight_decay: 0.01

# Optional learning rate scheduler
scheduler:
  name: warmup_cosine  # cosine | step | plateau | warmup_cosine
  warmup_epochs: 50
  max_epochs: ${training.max_epochs}
  eta_min: 1.0e-6

# Logging configuration
# Note: paradigm is auto-derived from ssl.name; referenced here via ${ssl.name}
logging:
  use_wandb: true
  wandb_project: ${project_name}
  wandb_entity: null  # Set to your W&B username/team
  run_name: pretrain_${dataset}_${ssl.name}_seed${seed}
  wandb_group: pretrain_${dataset}_${ssl.name}
  wandb_tags:
    - "phase:pretrain"
    - "dataset:${dataset}"
    - "paradigm:${ssl.name}"
    - "seed:${seed}"
  log_every_n_steps: 50

# Hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Deterministic mode (slower but reproducible)
deterministic: false
