# SMART SSL Pretraining Configuration
#
# SMART: Self-supervised Missing-Aware Reconstruction Transformer (NeurIPS 2024)
# Paper: https://openreview.net/forum?id=7UenF4kx4j
#
# Usage:
#   uv run python scripts/training/pretrain.py --config-name pretrain_smart
#   uv run python scripts/training/pretrain.py --config-name pretrain_smart training.max_epochs=100
#
# Or use the base pretrain.yaml with model/ssl overrides:
#   uv run python scripts/training/pretrain.py model=smart ssl=smart

defaults:
  - data: ricu
  - model: smart
  - ssl: smart
  - _self_

# Project info
project_name: slices
experiment_name: smart_pretrain

# Random seed for reproducibility
seed: 42

# Output directory (Hydra will create timestamped subdirs)
output_dir: ${hydra:runtime.output_dir}
checkpoint_dir: ${output_dir}/checkpoints

# Data configuration
data:
  num_workers: 8
  enable_sliding_windows: false
  window_size: 48
  window_stride: 24

# Encoder overrides (base from configs/model/smart.yaml)
# Only specify values that differ from the base config
encoder:
  pooling: none  # Required for SSL (4D output)

# SSL overrides (base from configs/ssl/smart.yaml)
# Base config already has good defaults matching the paper

# Training configuration (SMART paper defaults)
# NOTE: 25 epochs is the SMART paper default, but MAE uses 500 epochs in
# pretrain.yaml. For fair benchmark comparison, equalize training budget
# in gradient steps or wall-clock time rather than raw epochs.
training:
  max_epochs: 25          # Paper default for pretraining
  batch_size: 256         # Paper default
  accelerator: auto
  devices: auto
  precision: 32
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  early_stopping_patience: 10
  overfit_batches: 0

# Optimizer (Paper uses Adam with lr=1e-3)
optimizer:
  name: adamw
  lr: 1.0e-3
  weight_decay: 0.01

# Learning rate scheduler
scheduler:
  name: warmup_cosine
  warmup_epochs: 5
  max_epochs: ${training.max_epochs}
  eta_min: 1.0e-6

# Logging
logging:
  use_wandb: true
  wandb_project: ${project_name}
  wandb_entity: null
  run_name: ${experiment_name}_s${seed}
  wandb_group: ${experiment_name}
  log_every_n_steps: 50

# Hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

deterministic: false
