# Transformer encoder configuration for ICU time-series pretraining
#
# This config defines a modular transformer architecture for learning
# universal patient embeddings from unlabeled ICU time-series data.

name: transformer

encoder:
  # Input/Output dimensions
  d_input: 35  # Number of input features (core feature set: 9 vitals + 26 labs)
  d_model: 128  # Model dimension (embedding size)
  max_seq_length: 168  # Maximum sequence length in hours (7 days)

  # Transformer architecture
  n_layers: 4  # Number of transformer encoder layers
  n_heads: 8  # Number of attention heads (must divide d_model evenly)
  d_ff: 512  # Feedforward dimension (typically 4 * d_model)

  # Regularization
  dropout: 0.1  # Dropout probability for attention and feedforward

  # Architecture choices
  activation: gelu  # Activation function: gelu | relu | silu
  layer_norm_eps: 1.0e-5  # Layer normalization epsilon
  prenorm: true  # Pre-LN (true) vs Post-LN (false) transformer
  use_positional_encoding: true  # Add sinusoidal positional encoding

  # Pooling strategy for sequence-level representation
  # Options: mean | max | cls | last | none
  # - mean: Average over valid timesteps (recommended for SSL)
  # - max: Max pooling over valid timesteps
  # - cls: Use special [CLS] token (BERT-style)
  # - last: Use last valid timestep
  # - none: Return per-timestep embeddings (B, T, d_model)
  pooling: mean

  # Observation mask handling
  # When enabled, the observation mask is provided as explicit input to the model
  # instead of relying solely on forward-fill imputation.
  use_observation_mask: false  # Set to true for mask-aware training
  mask_input_mode: concat      # How to incorporate mask: concat (values + mask features)
  zero_imputed_values: false   # Zero out imputed values (let mask carry the info)

ssl:
  name: mae  # mae | jepa | contrastive
  # Objective-specific params added in subclasses

# Common configurations:
#
# Small (for debugging/testing):
#   d_model: 64, n_layers: 2, n_heads: 4, d_ff: 256
#   ~200K parameters
#
# Medium (default):
#   d_model: 128, n_layers: 4, n_heads: 8, d_ff: 512
#   ~1.5M parameters
#
# Large (research-scale):
#   d_model: 512, n_layers: 8, n_heads: 16, d_ff: 2048
#   ~25M parameters
