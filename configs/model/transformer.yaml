# Transformer encoder configuration

name: transformer

encoder:
  d_input: 35  # Number of input features (core feature set)
  d_model: 128  # Model dimension
  n_layers: 4  # Number of transformer layers
  n_heads: 8  # Number of attention heads
  dropout: 0.1
  max_seq_length: 168  # 7 days in hours

ssl:
  name: mae  # mae | jepa | contrastive
  # Objective-specific params added in subclasses

