# @package ssl
# MAE (Masked Autoencoder) SSL Configuration
#
# Two-Token System:
# This implementation uses two learned tokens to handle missing/masked values:
# - MISSING_TOKEN: Replaces genuinely missing positions (obs_mask=False)
# - MASK_TOKEN: Replaces SSL-masked positions (obs_mask=True AND selected for masking)
# Both tokens are learned parameters with shape (1, 1, d_input).
# Reconstruction loss is only computed on MASK_TOKEN positions.

name: mae

# Masking parameters
mask_ratio: 0.15  # Fraction of observed positions to mask (15% is BERT default)
mask_strategy: random  # random | block | timestep | feature
min_block_size: 3  # For block masking
max_block_size: 10  # For block masking

# Decoder parameters (lighter than encoder)
decoder_d_model: 64  # Decoder hidden dimension (smaller than encoder)
decoder_n_layers: 2  # Number of decoder transformer layers
decoder_n_heads: 4  # Number of decoder attention heads
decoder_d_ff: 256  # Decoder feedforward dimension
decoder_dropout: 0.1

# Loss parameters
loss_on_observed_only: true  # Only compute loss on originally observed values
norm_target: false  # Normalize reconstruction targets (per-sample)

# Notes:
# - random: BERT-style uniform random masking
# - block: Contiguous time blocks (better for temporal structure)
# - timestep: Mask entire timesteps across all features
# - feature: Mask entire features across all timesteps
#
# For ICU data, block masking often works best as it forces the model
# to learn temporal dependencies and interpolation.
