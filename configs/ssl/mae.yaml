# MAE (Masked Autoencoder) SSL Configuration

name: mae

# Masking parameters
mask_ratio: 0.15  # Fraction of input to mask (15% is BERT default)
mask_strategy: random  # random | block | timestep | feature
min_block_size: 3  # For block masking
max_block_size: 10  # For block masking
mask_value: 0.0  # Value to use for masked positions

# Decoder parameters (lighter than encoder)
decoder_d_model: 64  # Decoder hidden dimension (smaller than encoder)
decoder_n_layers: 2  # Number of decoder transformer layers
decoder_n_heads: 4  # Number of decoder attention heads
decoder_d_ff: 256  # Decoder feedforward dimension
decoder_dropout: 0.1

# Loss parameters
loss_on_observed_only: true  # Only compute loss on originally observed values
norm_target: false  # Normalize reconstruction targets (per-sample)

# Notes:
# - random: BERT-style uniform random masking
# - block: Contiguous time blocks (better for temporal structure)
# - timestep: Mask entire timesteps across all features
# - feature: Mask entire features across all timesteps
#
# For ICU data, block masking often works best as it forces the model
# to learn temporal dependencies and interpolation.
