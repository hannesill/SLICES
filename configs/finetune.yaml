# Downstream Task Finetuning Configuration
#
# This config orchestrates finetuning a pretrained encoder on downstream tasks.
# By default, the encoder is frozen and only the task head is trained (linear probing).
#
# Example usage:
#   # Linear probing (frozen encoder, default)
#   uv run python scripts/training/finetune.py dataset=miiv checkpoint=outputs/encoder.pt
#
#   # Full finetuning (train encoder + head)
#   uv run python scripts/training/finetune.py dataset=miiv checkpoint=outputs/encoder.pt training.freeze_encoder=false
#
#   # Gradual unfreezing (freeze for N epochs, then unfreeze)
#   uv run python scripts/training/finetune.py dataset=miiv checkpoint=outputs/encoder.pt training.unfreeze_epoch=5
#
#   # Different task
#   uv run python scripts/training/finetune.py dataset=miiv checkpoint=outputs/encoder.pt tasks=mortality_hospital
#
#   # Different dataset
#   uv run python scripts/training/finetune.py dataset=eicu checkpoint=outputs/encoder.pt

defaults:
  - data: ricu
  - model: observation_transformer
  - tasks: mortality_24h
  - _self_

# Project info
project_name: slices
experiment_name: finetune

# Dataset identifier — drives data paths via ricu.yaml interpolation
dataset: miiv  # miiv | eicu | combined

# Paradigm — auto-detected from checkpoint in finetune.py; override with paradigm=jepa etc.
paradigm: mae

# Experiment sprint (for W&B filtering, e.g. sprint=1)
sprint: null

# Random seed for reproducibility
seed: 42

# Checkpoint path (encoder weights from pretraining)
# Can be either:
#   - encoder.pt file: checkpoint=outputs/encoder.pt
#   - full .ckpt file: pretrain_checkpoint=outputs/ssl-epoch=099.ckpt
checkpoint: null
pretrain_checkpoint: null

# Output directory (Hydra will create timestamped subdirs)
output_dir: ${hydra:runtime.output_dir}
checkpoint_dir: ${output_dir}/checkpoints

# Encoder configuration inherited from configs/model/observation_transformer.yaml
# For SSL finetuning, the v3 checkpoint overrides this with the saved encoder config.

# Task configuration inherited from configs/tasks/*.yaml
# Override with: task=mortality_hospital, task=los_remaining, etc.

# Label fraction for label-efficiency ablations
# 1.0 = use all training data (default)
# Lower values (e.g., 0.01, 0.05, 0.1) subsample the training set
label_fraction: 1.0

# Training configuration
training:
  max_epochs: 50
  batch_size: 64

  # Encoder freezing strategy
  # freeze_encoder: true  -> Linear probing (only train head)
  # freeze_encoder: false -> Full finetuning (train encoder + head)
  freeze_encoder: true

  # Gradual unfreezing: unfreeze encoder after N epochs
  # Set to null to keep encoder frozen throughout training
  unfreeze_epoch: null

  # Use learned missing token from pretraining
  use_missing_token: true

  # Class weighting for imbalanced tasks
  # null: no weighting (default)
  # "balanced": auto-compute inverse frequency weights from training labels
  # [w0, w1, ...]: explicit per-class weights
  class_weight: null

  # Compute settings
  accelerator: auto  # auto | cpu | gpu | mps
  devices: auto  # auto | 1 | [0,1] | etc.
  precision: 32  # 32 | 16 | bf16

  # Regularization
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1

  # Early stopping
  # Monitor/mode are set automatically by task type in setup_callbacks():
  #   classification -> val/auprc (max), regression -> val/mse (min)
  # Override here only if you need a non-default metric.
  early_stopping_patience: 10

  # Debug: overfit on N batches (0 = disabled)
  overfit_batches: 0

# Optimizer configuration
# Note: Lower learning rate than pretraining for finetuning
optimizer:
  name: adamw
  lr: 1.0e-4  # Lower LR for finetuning
  weight_decay: 0.01

# Optional learning rate scheduler
scheduler:
  name: cosine
  T_max: ${training.max_epochs}
  eta_min: 1.0e-6

# Evaluation configuration
eval:
  fairness:
    enabled: false
    protected_attributes: [gender, age_group, race]
    min_subgroup_size: 50

# Logging configuration
logging:
  use_wandb: true
  wandb_project: ${project_name}
  wandb_entity: null  # Set to your W&B username/team
  run_name: finetune_${dataset}_${paradigm}_${task.task_name}_seed${seed}
  wandb_group: finetune_${dataset}_${paradigm}_${task.task_name}
  wandb_tags:
    - "phase:finetune"
    - "dataset:${dataset}"
    - "paradigm:${paradigm}"
    - "task:${task.task_name}"
    - "seed:${seed}"
  log_every_n_steps: 10

# Hydra configuration
hydra:
  run:
    dir: outputs/finetune/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/finetune/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

# Deterministic mode (slower but reproducible)
deterministic: false
